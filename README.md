# Awesome-Multimodal-LLM [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)
A curated list of papers related to multi-modal machine learning, especially multi-modal large language models (LLMs).

## Table of Contents
- [Datasets](#datasets)
- [Research Papers](#research-papers)
  - [Survey Papers](#survey-papers)
  - [Core Areas](#core-areas)
    - [Multimodal Understanding](#multimodal-understanding)
    - [Vision-Centric Understanding](#vision-centric-understanding)
    - [Embodied-Centric Understanding](#embodied-centric-understanding)
    - [Domain-Specific Models](#domain-specific-models)

# Datasets
[M3IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning](https://arxiv.org/abs/2306.04387), arxiv 2023 [[data]](https://huggingface.co/datasets/MMInstruction/M3IT)

[LLaVA Instruction 150K](https://llava-vl.github.io/), arxiv 2023 [[data]](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K)

# Research Papers

## Survey Papers

## Core Areas

### Multimodal Understanding
[InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://arxiv.org/abs/2305.06500), arxiv 2023 [[code]](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip) 

[BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597), ICML 2023 [[code]](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)

[Language Is Not All You Need: Aligning Perception with Language Models](https://arxiv.org/abs/2302.14045v2), arxiv 2023 [[code]](https://github.com/microsoft/unilm)

[ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities](http://arxiv.org/abs/2305.11172), arxiv 2023 [[code]](https://github.com/OFA-Sys/ONE-PEACE)

[X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages](https://arxiv.org/abs/2305.04160), arxiv 2023 [[code]](https://github.com/phellonchen/X-LLM)

[Visual Instruction Tuning](https://arxiv.org/abs/2304.08485), arxiv 2023 [[code]](https://github.com/haotian-liu/LLaVA)

[Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models](https://arxiv.org/abs/2303.04671), arxiv 2023 [[code]](https://github.com/microsoft/TaskMatrix)

[PaLI: A Jointly-Scaled Multilingual Language-Image Model](http://arxiv.org/abs/2209.06794), ICLR 2023 [[blog]](https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html)

[Grounding Language Models to Images for Multimodal Inputs and Outputs](https://arxiv.org/abs/2301.13823), ICML 2023 [[code]](https://github.com/kohjingyu/fromage)

[OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework](https://arxiv.org/abs/2202.03052), ICML 2022 [[code]](https://github.com/OFA-Sys/OFA)

[Flamingo: a Visual Language Model for Few-Shot Learning](https://arxiv.org/abs/2204.14198), NeurIPS 2022


### Vision-Centric Understanding
[Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding](https://arxiv.org/abs/2306.02858), arxiv 2023 [[code]](https://github.com/DAMO-NLP-SG/Video-LLaMA)

[SegGPT: Segmenting Everything In Context](http://arxiv.org/abs/2304.03284), arxiv 2023 [[code]](https://github.com/baaivision/Painter)

[VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks](http://arxiv.org/abs/2305.11175), arxiv 2023 [[code]](https://github.com/OpenGVLab/VisionLLM)

[Matcher: Segment Anything with One Shot Using All-Purpose Feature Matching](https://arxiv.org/abs/2305.13310), arxiv 2023

[Personalize Segment Anything Model with One Shot](https://arxiv.org/abs/2305.03048), arxiv 2023 [[code]](https://github.com/ZrrSkywalker/Personalize-SAM)

[Segment Anything](https://arxiv.org/abs/2304.02643), arxiv 2023 [[code]](https://github.com/facebookresearch/segment-anything)

[Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks](https://arxiv.org/abs/2211.09808), CVPR 2023 [[code]](https://github.com/fundamentalvision/Uni-Perceiver)

[A Generalist Framework for Panoptic Segmentation of Images and Videos](https://arxiv.org/abs/2210.06366), arxiv 2022 

[A Unified Sequence Interface for Vision Tasks](http://arxiv.org/abs/2206.07669), NeurIPS 2022 [[code]](https://github.com/google-research/pix2seq)

[Pix2seq: A language modeling framework for object detection](https://arxiv.org/abs/2109.10852), ICLR 2022 [[code]](https://github.com/google-research/pix2seq)


### Embodied-Centric Understanding
[PaLM-E: An Embodied Multimodal Language Model](https://arxiv.org/abs/2303.03378), arxiv 2023 [[blog]](https://palm-e.github.io/)

[Generative Agents: Interactive Simulacra of Human Behavior](https://arxiv.org/abs/2304.03442), arxiv 2023

[Vision-Language Models as Success Detectors](https://arxiv.org/abs/2303.07280), arxiv 2023

[TidyBot: Personalized Robot Assistance with Large Language Models](https://arxiv.org/abs/2305.05658), arxiv 2023 [[code]](https://github.com/jimmyyhwu/tidybot)

[Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action](https://arxiv.org/abs/2207.04429), CoRL 2022 [[blog]](https://sites.google.com/view/lmnav) [[code]](https://github.com/blazejosinski/lm_nav)



### Domain-Specific Models
[LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day](http://arxiv.org/abs/2306.00890), arxiv 2023 [[code]](https://github.com/microsoft/LLaVA-Med)
